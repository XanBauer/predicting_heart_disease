{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Presence of Heart Disease with Logistic Regression \n",
    "\n",
    "## Table of contents <a name= \"toc\"></a>\n",
    "1. [Introduction](#introduction)\n",
    "2. [Preprocessing and Feature Engineering](#preprocessing)\n",
    "    1. [Exploratory Data Analysis](#eda)\n",
    "    2. [Preprocessing summary](#ppsummary)\n",
    "3. [Finding Relationships](#relationships)\n",
    "4. [Logistic Regression](#log)\n",
    "    1. [Understanding Logistic Regression](#understand)\n",
    "    2. [Creating the Model](#create)\n",
    "    3. [Training the Model](#train)\n",
    "    4. [Testing the Model and Analyzing Results](#test)\n",
    "5. [Tuning the Model](#tune)\n",
    "    1. [Analyzing Effects of Tuning](#tune_effects)\n",
    "6. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction <a name= \"introduction\"></a>\n",
    "\n",
    "Machine learning to save lives presents the power and potential for change in the world around us. With the Heart Disease data set from [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/45/heart+disease), we can create a classification model to predict the presence of heart disease and help people get life saving care before it's too late. \n",
    "\n",
    "UCI has created a python package to easily pull and interact with the data. Let's start by pulling and preprocessing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries at beginning so future cells can be run more efficiently \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Formatting\n",
    "import pprint\n",
    "from rich.console import Console\n",
    "console = Console()\n",
    "from rich.table import Table\n",
    "\n",
    "# Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# Maths\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from math import e\n",
    "\n",
    "# import visualization library and set style\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "sns.set_color_codes(\"muted\")\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset \n",
    "heart_disease = fetch_ucirepo(id=45) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = heart_disease.data.features \n",
    "y = heart_disease.data.targets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Engineering <a name= \"preprocessing\"></a>\n",
    "[Back to Table of Contents](#toc)\n",
    "\n",
    "The link to the dataset has returned a wealth of information. Let's highlight the most important parts\n",
    "- We have 303 records for analysis\n",
    "- There are 13 features to use for classification\n",
    "- Of the 13 features, they are either Categorical or Integer types\n",
    "- The target column is \"num\". 0 = no heart disease where 1-4 = varying levels of heart disease\n",
    "- There are missing values denoted by NaN in the columns \"ca\" and \"thal\". We will look to impute these values later on\n",
    "- Finally, these column names are abbreviated and specific to cardiology. We can rename these fields for ease of understanding\n",
    "\n",
    "First, we will transform the target column into binary values rather than multiple class classification. This helps us answer the big question of \"is a heart disease present?\" rather than the more nuanced question of \"is a heart disease present, and if so, what is it?\". The doctors can take care of that part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num\n",
      "0      164\n",
      "1       55\n",
      "2       36\n",
      "3       35\n",
      "4       13\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Confirm all present values in target column before processing\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num\n",
      "0    164\n",
      "1    139\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Adjust all non-zero values to be 1\n",
    "y = y[\"num\"].apply(lambda x: 0 if x==0 else 1)\n",
    "\n",
    "# Confirm correct values after transformation\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let\"s rename the columns for legibility. The trade off is that code and results can look clunkier, but it's more important that we understand the data. We will dive into the data of the columns later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map = {\n",
    "    \"age\": \"age\",\n",
    "    \"sex\": \"male\",\n",
    "    \"cp\": \"chest_pain\",\n",
    "    \"trestbps\": \"resting_blood_pressure\",\n",
    "    \"chol\": \"cholesterol\",\n",
    "    \"fbs\": \"fasting_blood_sugar\",\n",
    "    \"restecg\": \"rest_ecg\",\n",
    "    \"thalach\": \"max_heart_rate\",\n",
    "    \"exang\": \"exercise_induced_pain\",\n",
    "    \"oldpeak\": \"st_depression\",\n",
    "    \"slope\": \"st_slope\",\n",
    "    \"ca\": \"colored_arteries\",\n",
    "    \"thal\": \"thalassemia\"\n",
    "}\n",
    "\n",
    "X = X.rename(columns=column_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let\"s find and address missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     age  male  chest_pain  resting_blood_pressure  cholesterol  \\\n",
      "87    53     0           3                     128          216   \n",
      "166   52     1           3                     138          223   \n",
      "192   43     1           4                     132          247   \n",
      "266   52     1           4                     128          204   \n",
      "287   58     1           2                     125          220   \n",
      "302   38     1           3                     138          175   \n",
      "\n",
      "     fasting_blood_sugar  rest_ecg  max_heart_rate  exercise_induced_pain  \\\n",
      "87                     0         2             115                      0   \n",
      "166                    0         0             169                      0   \n",
      "192                    1         2             143                      1   \n",
      "266                    1         0             156                      1   \n",
      "287                    0         0             144                      0   \n",
      "302                    0         0             173                      0   \n",
      "\n",
      "     st_depression  st_slope  colored_arteries  thalassemia  num  \n",
      "87             0.0         1               0.0          NaN    0  \n",
      "166            0.0         1               NaN          3.0    0  \n",
      "192            0.1         2               NaN          7.0    1  \n",
      "266            1.0         2               0.0          NaN    1  \n",
      "287            0.4         2               NaN          7.0    0  \n",
      "302            0.0         1               NaN          3.0    0  \n"
     ]
    }
   ],
   "source": [
    "# Find records with any missing NaN values\n",
    "missing_vals_index = (X[X.isna().any(axis=1)]).index\n",
    "\n",
    "# Combine missing these records with the same in the target column\n",
    "print(pd.concat([X.iloc[missing_vals_index], y.iloc[missing_vals_index]], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is a small amount of data in this dataset (303 records) and few rows (6) with only one missing value each, we will use imputation. Imputation fills missing values with the mode or mean value from that field. \n",
    "\n",
    "Imputation preserves valuable data from the rest of the record without introducing a significant risk of confounding the data.\n",
    "\n",
    "Because we have data for case and non case values, let's impute the missing values with the `ca` mean (integer feature) and `thal` mode (categorical feature) for the respective section of the dataset with the same target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case instances\n",
      "Mean colored_arteries: 1.0\n",
      "Mode thalassemia: 7.0\n",
      "non-case instances\n",
      "Mean colored_arteries: 0.0\n",
      "Mode thalassemia: 3.0\n"
     ]
    }
   ],
   "source": [
    "# Isolate rows for case values\n",
    "case_index = y[y==1].index\n",
    "print(\"case instances\")\n",
    "print(f\"Mean colored_arteries: {round(X.loc[case_index,:]['colored_arteries'].mean(),0)}\")\n",
    "print(f\"Mode thalassemia: {X.loc[case_index,:]['thalassemia'].mode()[0]}\")\n",
    "\n",
    "# Isolate rows for non case values\n",
    "non_case_index = y[y==0].index\n",
    "print(\"non-case instances\")\n",
    "print(f\"Mean colored_arteries: {round(X.loc[non_case_index,:]['colored_arteries'].mean(),0)}\")\n",
    "print(f\"Mode thalassemia: {X.loc[non_case_index,:]['thalassemia'].mode()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are only a few values to replace and we do not need to repeat this, we will manually update these values. If that were not the case, we would create a function to update these values dependent on the column and case value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.loc[87,\"thalassemia\"] = 3.0\n",
    "X.loc[166, \"colored_arteries\"] = 0.0\n",
    "X.loc[192, \"colored_arteries\"] = 1.0\n",
    "X.loc[266, \"thalassemia\"] = 7.0\n",
    "X.loc[287, \"colored_arteries\"] = 0.0\n",
    "X.loc[302, \"colored_arteries\"] = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis <a name= \"eda\"></a>\n",
    "\n",
    "Now that the data is cleaned, we will organize the variables into categorical and integer features and visualize the data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate categorical and integer features\n",
    "cat_feats = [\"male\", \"chest_pain\", \"fasting_blood_sugar\", \"rest_ecg\", \"exercise_induced_pain\", \"st_slope\", \"thalassemia\"]\n",
    "\n",
    "# Create histograms of categorical features\n",
    "X[cat_feats].hist(figsize=(12,10))\n",
    "\n",
    "# Save figure instead of displaying it inline for memory efficiency\n",
    "plt.savefig(\"images/categorical_eda.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Clear the plot to free memory\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://raw.githubusercontent.com/XanBauer/predicting_heart_disease/main/images/categorical_eda.png\" width=\"900\" height=\"750\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the categorical variables:\n",
    "* `male`: More than twice as many men are present in this dataset than women, which can bias the model towards better performance with males\n",
    "    * **0**: female\n",
    "    * **1**: male\n",
    "* `chest_pain`: represents the type of chest pain felt by the patient. Most commonly asymptomatic\n",
    "    * **1**: typical angina (typical heart pain)\n",
    "    * **2**: atypical angina (atypical heart pain)\n",
    "    * **3**: non-anginal pain (chest pain unrelated to heart disease)\n",
    "    * **4**: asymptomatic (no chest pain)\n",
    "* `fasting_blood_sugar`: represents whether the patient's fasting blood sugar >120mg/dl. According to [Mount Sinai Hospital](https://mountsinai.org/health-library/tests/blood-sugar-test-blood#:~:text=A%20level%20of%20100%20to,usually%20means%20you%20have%20diabetes.), this level of fasting blood sugar indicates presence of prediabetes or diabetes, which could correlate with presence of heart disesease\n",
    "    * **0**: false\n",
    "    * **1**: true\n",
    "* `rest_ecg`: represents ecg results at rest\n",
    "    * **0**: normal\n",
    "    * **1**: 1 is common and [not necessarily indicative of an issue](https://www.uptodate.com/contents/ecg-tutorial-st-and-t-wave-changes#:~:text=ST%2D%20and%20T%2Dwave%20changes%20may%20represent%20cardiac%20pathology%20or,similar%20findings%20on%20prior%20electrocardiograms.&text=Nonspecific%20ST%2DT%2Dwave%20changes%20are%20very%20common%20and%20may,any%20lead%20of%20the%20electrocardiogram.)\n",
    "    * **2**: probable to definite sign of thickening heart muscles, making it [harder to pump blood](https://www.mayoclinic.org/diseases-conditions/left-ventricular-hypertrophy/symptoms-causes/syc-20374314) and increasing the risk of heart disease\n",
    "* `exercise_induced_pain`: signifies if the chest pain was brought on by exercise, which looks to be about 30% of the records and is likely higher for instances when heart disease is present\n",
    "    * **0**: false\n",
    "    * **1**: true\n",
    "* `st_slope`: this represents shape  an ST slope and can be an important marker in [reduced blood flow](https://www.ajconline.org/article/S0002-9149(01)02052-5/abstract#:~:text=ST%20slope%20is%20a%20sensitive,even%20better%20than%20ST%20depression.) to the heart\n",
    "    * **0**: upsloping\n",
    "    * **1**: flat\n",
    "    * **2**: downsloping\n",
    "* `thalassemia`: While not defined in the data dictionary, `thatl` likel refers to a blood disorder called Thalassemia which limits how much oxygen can be carried in the blood.\n",
    "    * **3**: normal\n",
    "    * **6**: fixed defect\n",
    "    * **7**: reversable defect\n",
    "\n",
    "\n",
    "Next, we will OHE (one hot encode) the categorical variables with more than two classes because the logistic regression model will assume a scalar relationship between the values. For example, a value of 3 in the `slope` variable is not three times more than a value of `1`, but the logistic model will treat it as such. \n",
    "\n",
    "With OHE, it is best practice to drop one of the columns to avoid the problem of multicollinearity. This is when two or more independent variables are highly correlated with each other. As a result, the causal relationships between the indepent variables and the target variable can be unclear and the model will overfit. \n",
    "\n",
    "Logistic regression assumes independence between independent variables. If we waited to drop variables until after testing for relationships between them and the target variable, it could result in dropping columns that do not affect the independence between the remaining categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create OHE values for the necessary categorical features\n",
    "one_hot_cols = [\"chest_pain\", \"rest_ecg\",\"st_slope\",\"thalassemia\"]\n",
    "\n",
    "X_encoded = X.copy()\n",
    "\n",
    "# Replace orginal columns with OHE columns\n",
    "for col in one_hot_cols:\n",
    "    # Create one hot columns\n",
    "    encoded = pd.get_dummies(X_encoded[col], prefix= col, drop_first= True)\n",
    "    \n",
    "    # Update df and categorical features list\n",
    "    X_encoded = X_encoded.drop(columns= col)\n",
    "    cat_feats.remove(col)\n",
    "    \n",
    "    X_encoded = pd.concat([X_encoded, encoded], axis= 1)\n",
    "    for new_col in encoded.columns:\n",
    "        cat_feats.append(new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_feats = [\"age\", \"resting_blood_pressure\", \"cholesterol\", \"max_heart_rate\", \"st_depression\", \"colored_arteries\"]\n",
    "\n",
    "# Create histograms of integer features\n",
    "plots = X_encoded[int_feats].hist(figsize=(12,10))\n",
    "\n",
    "# Save figure instead of displaying it inline for memory efficiency\n",
    "plt.savefig(\"images/integer_eda.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Clear the plot to free memory\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://raw.githubusercontent.com/XanBauer/predicting_heart_disease/main/images/integer_eda.png\" width=\"900\" height=\"750\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the integer variables\n",
    "\n",
    "- `age`: The distribution is negatively skewed, showing that there are more instances of older people present in this dataset\n",
    "- `resting_blood_pressure`: resting blood pressure, of which a healthy range is different by sex and age\n",
    "- `cholesterol`: serum cholesterol\n",
    "- `max_heart_rate`: maximum heart rate achieved\n",
    "- `st_depression`: \"ST depression induced by exercise relative to rest\". ST depression is a finding on an ECG which measures the rhythm and electrical activity in the heart.  The presence of an ST depression [can indicate potentially life threatening disease](https://www.medicalnewstoday.com/articles/st-depression-on-ecg#:~:text=An%20ECG%20is%20a%20test,reviewed%20on%20April%2025%2C%202024)\n",
    "- `colored_arteries`: represents the number of major vessels colored by flourosopy and can identify [narrowing or blockages in artieries](https://my.clevelandclinic.org/health/diagnostics/21992-fluoroscopy#test-details)\n",
    "\n",
    "Because this dataset has variables with different scales, something logistic regression is sensitive to, we will use standard scaling to avoid an overemphasis on variables with larger scales and vice versa. Standard scaling transforms data to have normal distribution with the mean at 0 and values measured in standard deviations (SD) away from the mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Create copy to preserve orginal data\n",
    "X_scaled = X_encoded.copy()\n",
    "\n",
    "# Fit and transform integer features\n",
    "scaled_features = scaler.fit_transform(X[int_feats])\n",
    "\n",
    "# Update X with scaled features\n",
    "X_scaled[int_feats] = scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing summary <a name= \"ppsummary\"></a>\n",
    "\n",
    "We did multiple things to prepare the dataset for analysis. We simplified the target variable into \"does the patient have heart disease or not?\", we made the column names easier to understand, we OHE categorical features and standard scaled numerical features so that they're appropriate for logistic regression.\n",
    "\n",
    "## Finding relationships <a name= \"relationships\"></a>\n",
    "[Back to Table of Contents](#toc)\n",
    "\n",
    "Now that the features have been prepared, we are ready to look for relationships between them and the target variable. \n",
    "\n",
    "We will start with using a chi square test on the categorical features. Chi square tests operate on the assumption that there is no significant difference between the proportions of observed instances amongst the tested categories. If there is a significant difference, this points to a relationship that can be useful for building our logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'male': (np.float64(22.04), np.float64(2.666712348180942e-06), np.True_),\n",
      " 'fasting_blood_sugar': (np.float64(0.08),\n",
      "                         np.float64(0.7812734067063785),\n",
      "                         np.False_),\n",
      " 'exercise_induced_pain': (np.float64(54.69),\n",
      "                           np.float64(1.413788096718085e-13),\n",
      "                           np.True_),\n",
      " 'chest_pain_2': (np.float64(17.42),\n",
      "                  np.float64(2.999461848863365e-05),\n",
      "                  np.True_),\n",
      " 'chest_pain_3': (np.float64(28.71),\n",
      "                  np.float64(8.425565836925393e-08),\n",
      "                  np.True_),\n",
      " 'chest_pain_4': (np.float64(78.76),\n",
      "                  np.float64(7.023529768902272e-19),\n",
      "                  np.True_),\n",
      " 'rest_ecg_1': (np.float64(0.45), np.float64(0.5017489946446447), np.False_),\n",
      " 'rest_ecg_2': (np.float64(7.16), np.float64(0.007434268855662301), np.True_),\n",
      " 'st_slope_2': (np.float64(36.92), np.float64(1.230960081661359e-09), np.True_),\n",
      " 'st_slope_3': (np.float64(0.72), np.float64(0.39687881496027133), np.False_),\n",
      " 'thalassemia_6.0': (np.float64(2.5),\n",
      "                     np.float64(0.11376652914688604),\n",
      "                     np.False_),\n",
      " 'thalassemia_7.0': (np.float64(69.93),\n",
      "                     np.float64(6.158409596935525e-17),\n",
      "                     np.True_)}\n"
     ]
    }
   ],
   "source": [
    "# Create function to run chi square test\n",
    "def chi_square(cat_col):\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(cat_col, y)\n",
    "\n",
    "    # Performing the Chi-Square Test\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "    # Interpreting the result\n",
    "    significant = p < 0.05  # Standard 5% significance level\n",
    "    return round(chi2,2), p, significant\n",
    "\n",
    "# Run the chi_square function for each of the categorical features\n",
    "chi_square_results = {}\n",
    "for col in cat_feats:\n",
    "    chi_square_results[col] = chi_square(X_scaled[col])\n",
    "pprint.pp(chi_square_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The results of the chi square tests show that most of the features have a significant relationship with the presence of heart disease. \n",
    "\n",
    "The chi square value measures the diference between the observed and expected frequencies while the p-value represents the percentage chance that the observed relationship is due to chance rather than a connection between the variables. A higher chi square value and a lower p value represent stronger relationships.\n",
    "\n",
    "We will remove columns that did not have a significant p value, as they are more likely to weaken the predictive model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in chi_square_results:\n",
    "    if chi_square_results[k][2] == False:\n",
    "        X_scaled = X_scaled.drop(columns=k)\n",
    "        cat_feats.remove(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can move on to finding relationships between the integer variables and the target variable. With some online research, I found multiple tests to run so we can create a stronger understanding of the relationships between the variables.\n",
    "\n",
    "Each test tells us something different:\n",
    "\n",
    "* Point-Biserial Correlation shows the linear relationship strength between the independent and target variable (-1 to 1)\n",
    "* Mann-Whitney U test compares distributions and shows whether the distributions differ significantly\n",
    "* Cohen\"s d measures effect size between two groups by expressing the difference between their means in standard deviation units\n",
    "* Logistic Regression Single Feature Analysis shows the predictive power of the indepdent variable (AUC-ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of significant features: 6\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                 Significant Features Summary                                  </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Feature                </span>┃<span style=\"font-weight: bold\"> Correlation       </span>┃<span style=\"font-weight: bold\"> Effect Size        </span>┃<span style=\"font-weight: bold\"> AUC-ROC </span>┃<span style=\"font-weight: bold\"> Mean Difference </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ age                    │ 0.223 (Weak)      │ 0.461 (Small)      │ 0.637   │ 0.448           │\n",
       "│ resting_blood_pressure │ 0.151 (Weak)      │ 0.303 (Small)      │ 0.574   │ 0.303           │\n",
       "│ cholesterol            │ 0.085 (Weak)      │ 0.172 (Negligible) │ 0.570   │ 0.171           │\n",
       "│ max_heart_rate         │ -0.417 (Moderate) │ -0.912 (Large)     │ 0.745   │ -0.837          │\n",
       "│ st_depression          │ 0.425 (Moderate)  │ 0.919 (Large)      │ 0.735   │ 0.852           │\n",
       "│ colored_arteries       │ 0.464 (Moderate)  │ 1.029 (Large)      │ 0.753   │ 0.932           │\n",
       "└────────────────────────┴───────────────────┴────────────────────┴─────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                 Significant Features Summary                                  \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mFeature               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mCorrelation      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mEffect Size       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAUC-ROC\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mMean Difference\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ age                    │ 0.223 (Weak)      │ 0.461 (Small)      │ 0.637   │ 0.448           │\n",
       "│ resting_blood_pressure │ 0.151 (Weak)      │ 0.303 (Small)      │ 0.574   │ 0.303           │\n",
       "│ cholesterol            │ 0.085 (Weak)      │ 0.172 (Negligible) │ 0.570   │ 0.171           │\n",
       "│ max_heart_rate         │ -0.417 (Moderate) │ -0.912 (Large)     │ 0.745   │ -0.837          │\n",
       "│ st_depression          │ 0.425 (Moderate)  │ 0.919 (Large)      │ 0.735   │ 0.852           │\n",
       "│ colored_arteries       │ 0.464 (Moderate)  │ 1.029 (Large)      │ 0.753   │ 0.932           │\n",
       "└────────────────────────┴───────────────────┴────────────────────┴─────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_numeric_binary_relationships(\n",
    "    df, \n",
    "    numeric_columns, \n",
    "    target_column,\n",
    "    alpha=0.05,\n",
    "    multiple_testing_method=\"fdr_bh\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of relationships between numeric features and binary target.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing features and target\n",
    "    numeric_columns : list\n",
    "        List of numeric column names to analyze\n",
    "    target_column : str\n",
    "        Name of binary target column\n",
    "    alpha : float, default=0.05\n",
    "        Significance level for hypothesis tests\n",
    "    multiple_testing_method : str, default=\"fdr_bh\"\n",
    "        Multiple testing correction method\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict containing:\n",
    "        - summary_df: DataFrame with all test results\n",
    "        - figures: Dictionary of generated plots\n",
    "        - significant_features: List of significant features\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    figures = {}\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        # 1. Point-Biserial Correlation\n",
    "        correlation, pb_pvalue = stats.pointbiserialr(\n",
    "            df[col],\n",
    "            df[target_column]\n",
    "        )\n",
    "        \n",
    "        # 2. Mann-Whitney U Test\n",
    "        mw_statistic, mw_pvalue = stats.mannwhitneyu(\n",
    "            df[df[target_column] == 0][col],\n",
    "            df[df[target_column] == 1][col],\n",
    "            alternative=\"two-sided\"\n",
    "        )\n",
    "        \n",
    "        # 3. Effect Size (Cohen\"s d)\n",
    "        group0 = df[df[target_column] == 0][col]\n",
    "        group1 = df[df[target_column] == 1][col]\n",
    "        cohens_d = (group1.mean() - group0.mean()) / np.sqrt(\n",
    "            ((group1.std() ** 2 + group0.std() ** 2) / 2)\n",
    "        )\n",
    "        \n",
    "        # 4. Logistic Regression Single Feature\n",
    "        model = LogisticRegression(random_state=42)\n",
    "        model.fit(df[[col]], df[target_column])\n",
    "        auc = roc_auc_score(\n",
    "            df[target_column], \n",
    "            model.predict_proba(df[[col]])[:, 1]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Store all results\n",
    "        results.append({\n",
    "            \"feature\": col,\n",
    "            \"correlation\": correlation,\n",
    "            \"pb_pvalue\": pb_pvalue,\n",
    "            \"mw_pvalue\": mw_pvalue,\n",
    "            \"cohens_d\": cohens_d,\n",
    "            \"auc_roc\": auc,\n",
    "            \"mean_target_0\": group0.mean(),\n",
    "            \"mean_target_1\": group1.mean(),\n",
    "            \"std_target_0\": group0.std(),\n",
    "            \"std_target_1\": group1.std()\n",
    "        })\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Apply multiple testing correction\n",
    "    _, pb_corrected_pvalues, _, _ = multipletests(\n",
    "        summary_df[\"pb_pvalue\"], \n",
    "        alpha=alpha, \n",
    "        method=multiple_testing_method\n",
    "    )\n",
    "    _, mw_corrected_pvalues, _, _ = multipletests(\n",
    "        summary_df[\"mw_pvalue\"], \n",
    "        alpha=alpha, \n",
    "        method=multiple_testing_method\n",
    "    )\n",
    "    \n",
    "    summary_df[\"pb_corrected_pvalue\"] = pb_corrected_pvalues\n",
    "    summary_df[\"mw_corrected_pvalue\"] = mw_corrected_pvalues\n",
    "    \n",
    "    # Add interpretation columns\n",
    "    summary_df[\"correlation_strength\"] = summary_df[\"correlation\"].apply(\n",
    "        lambda x: \"Strong\" if abs(x) > 0.5 \n",
    "        else \"Moderate\" if abs(x) > 0.3 \n",
    "        else \"Weak\"\n",
    "    )\n",
    "    \n",
    "    summary_df[\"effect_size_interpretation\"] = summary_df[\"cohens_d\"].apply(\n",
    "        lambda x: \"Large\" if abs(x) > 0.8 \n",
    "        else \"Medium\" if abs(x) > 0.5 \n",
    "        else \"Small\" if abs(x) > 0.2 \n",
    "        else \"Negligible\"\n",
    "    )\n",
    "    \n",
    "    # Identify significant features\n",
    "    significant_features = summary_df[\n",
    "        (summary_df[\"pb_corrected_pvalue\"] < alpha) | \n",
    "        (summary_df[\"mw_corrected_pvalue\"] < alpha)\n",
    "    ][\"feature\"].tolist()\n",
    "    \n",
    "    return {\n",
    "        \"summary_df\": summary_df,\n",
    "        \"significant_features\": significant_features\n",
    "    }\n",
    "\n",
    "def print_analysis_summary(analysis_results):\n",
    "    \"\"\"\n",
    "    Print summary of the analysis results.\n",
    "    \"\"\"\n",
    "    summary_df = analysis_results[\"summary_df\"]\n",
    "    significant = analysis_results[\"significant_features\"]\n",
    "    \n",
    "    print(f\"Number of significant features: {len(significant)}\\n\")\n",
    "    \n",
    "    if significant:\n",
    "        summary_table = Table(title=\"Significant Features Summary\")\n",
    "\n",
    "        summary_table.add_column(\"Feature\", justify=\"left\", no_wrap=True)\n",
    "        summary_table.add_column(\"Correlation\")\n",
    "        summary_table.add_column(\"Effect Size\")\n",
    "        summary_table.add_column(\"AUC-ROC\")\n",
    "        summary_table.add_column(\"Mean Difference\")\n",
    "        \n",
    "        for feature in significant:\n",
    "            row = summary_df[summary_df[\"feature\"] == feature].iloc[0]\n",
    "            summary_table.add_row(f\"{feature}\", f\"{row['correlation']:.3f} ({row['correlation_strength']})\", f\"{row['cohens_d']:.3f} ({row['effect_size_interpretation']})\",f\"{row['auc_roc']:.3f}\",f\"{row['mean_target_1'] - row['mean_target_0']:.3f}\")\n",
    "\n",
    "    \n",
    "    return console.print(summary_table)\n",
    "\n",
    "analysis_results = analyze_numeric_binary_relationships(\n",
    "    pd.concat([X_scaled, y], axis=1), \n",
    "    int_feats, \n",
    "    \"num\")\n",
    "print_analysis_summary(analysis_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `age`, `resting_blood_pressure`, and `cholesterol` have significant relationships to the presence of heart disease, the strength of that relationship and the effect they have on the presence of heart disease is small. Of these three, `age` appears to be the strongest predictor of heart disease. \n",
    "\n",
    "Compare that to `max_heart_rate`, `st_depression`, and `colored_arteries`, which all have much stronger relationships to the presence of heart disease. \n",
    "\n",
    "For the logistic regression model, we will use all of these variables. However, we can use this information to inform patients that their likelihood of developing heart disease is more likely to be affected by things they can control like increasing their max heart rate with exercise and keeping their arteries clear by eating healthy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression <a name= \"log\"></a>\n",
    "[Back to table of contents](#toc)\n",
    "\n",
    "### Understanding Logistic Regression <a name= \"understand\"></a>\n",
    "Logistic regression is a helpful model for this situation because it allows us to input multiple variables and result in a yes/no (or binary) prediciton. \n",
    "\n",
    "To do this, we transform the input variables into probability between 0-1 using the \"logistic\" or \"sigmoid\" function. \n",
    "\n",
    "$$P(EY) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1)}}$$\n",
    "\n",
    "Where $P(EY)$ is the probability event Y occurs.\n",
    "\n",
    "This results in the curve below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example data\n",
    "ex_x = np.linspace(-10, 10, 100)\n",
    "ex_y = 1 / (1 + np.exp(-ex_x))\n",
    "\n",
    "# Plot logistic function\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ex_x, ex_y)\n",
    "plt.title('The Logistic Function: An S-shaped Curve')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=0, color='r', linestyle='--', alpha=0.3)\n",
    "plt.grid(False)\n",
    "\n",
    "# Save figure instead of displaying it inline for memory efficiency\n",
    "plt.savefig(\"images/sigmoid_function.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Clear the plot to free memory\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://raw.githubusercontent.com/XanBauer/predicting_heart_disease/main/images/sigmoid_function.png\" width=\"750\" height=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The S-Shaped curve fits binary classification for multiple reasons\n",
    " * The output is bounded between 0 and 1, no matter how extreme the inputs. This ensures that we always get a valid probability\n",
    " * The curve changes gradually, then very quickly. This reflects how things can shift very quickly in the nexus of situation, but as we near an outcome, probabilities change less\n",
    "\n",
    "To do this, logistic regression transforms input data into predictions with three elements:\n",
    " 1. Input features (the data)\n",
    " 2. Weights (what the model learns)\n",
    " 3. The logistic function (the S-curve transformer)\n",
    " \n",
    "First, the features are combined linearly\n",
    "\n",
    "$$z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$\n",
    "\n",
    "Then, this is transformed into a probability using the sigmoid function\n",
    "\n",
    "$$P(EY) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "Now that we understand how the function works, we can build the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating the Model <a name= \"create\"></a>\n",
    "\n",
    "Now that we have identified which variables have a significant relationship with the presence of heart disease, we can set to work on creating our logistic regression model that will allow us to better predict whether a patient has a heart disease. To ensure the model accurately reflects the data set, we will split train and test data to contain proportions reflective of the case vs non case values.\n",
    "\n",
    "After we transformed our data, we saw that there are 164 non-case:139 case values (54%:46%). To maintain this ratio, we will use the stratify parameter in train_test_split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train class balance:\n",
      "num\n",
      "0    54.19\n",
      "1    45.81\n",
      "Name: proportion, dtype: float64\n",
      "y_test class balance:\n",
      "num\n",
      "0    53.95\n",
      "1    46.05\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split data with standard 25% test size\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=24, stratify=y)\n",
    "\n",
    "# Ensure that both test and train sets both use cases\n",
    "print(\"y_train class balance:\")\n",
    "print(round(y_train.value_counts(normalize=True)*100,2))\n",
    "print(\"y_test class balance:\")\n",
    "print(round(y_test.value_counts(normalize=True)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model <a name= \"train\"></a>\n",
    "\n",
    "Now that we have confirmed the splits have similar class balances to the whole dataset, we can move on to creating and training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 records analyzed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              Model Results              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric                       </span>┃<span style=\"font-weight: bold\"> Result </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Intercept                    │ 0.0841 │\n",
       "│ Coef: age                    │ -0.18  │\n",
       "│ Coef: male                   │ 1.07   │\n",
       "│ Coef: resting_blood_pressure │ 0.34   │\n",
       "│ Coef: cholesterol            │ 0.28   │\n",
       "│ Coef: max_heart_rate         │ -0.55  │\n",
       "│ Coef: exercise_induced_pain  │ 0.76   │\n",
       "│ Coef: st_depression          │ 0.2    │\n",
       "│ Coef: colored_arteries       │ 1.1    │\n",
       "│ Coef: chest_pain_2           │ 0.53   │\n",
       "│ Coef: chest_pain_3           │ -0.53  │\n",
       "│ Coef: chest_pain_4           │ 1.33   │\n",
       "│ Coef: rest_ecg_2             │ 0.17   │\n",
       "│ Coef: st_slope_2             │ 0.7    │\n",
       "│ Coef: thalassemia_7.0        │ 1.22   │\n",
       "└──────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m              Model Results              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mResult\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Intercept                    │ 0.0841 │\n",
       "│ Coef: age                    │ -0.18  │\n",
       "│ Coef: male                   │ 1.07   │\n",
       "│ Coef: resting_blood_pressure │ 0.34   │\n",
       "│ Coef: cholesterol            │ 0.28   │\n",
       "│ Coef: max_heart_rate         │ -0.55  │\n",
       "│ Coef: exercise_induced_pain  │ 0.76   │\n",
       "│ Coef: st_depression          │ 0.2    │\n",
       "│ Coef: colored_arteries       │ 1.1    │\n",
       "│ Coef: chest_pain_2           │ 0.53   │\n",
       "│ Coef: chest_pain_3           │ -0.53  │\n",
       "│ Coef: chest_pain_4           │ 1.33   │\n",
       "│ Coef: rest_ecg_2             │ 0.17   │\n",
       "│ Coef: st_slope_2             │ 0.7    │\n",
       "│ Coef: thalassemia_7.0        │ 1.22   │\n",
       "└──────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">             Model Scores             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric                    </span>┃<span style=\"font-weight: bold\"> Result </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Accuracy                  │ 85.0   │\n",
       "│ Sensitivity               │ 81.0   │\n",
       "│ Specificity               │ 89.0   │\n",
       "│ Positive Predictive Value │ 87.0   │\n",
       "│ Negative Predictive Value │ 85.0   │\n",
       "│ False Positive Rate       │ 11.0   │\n",
       "│ False Negative Rate       │ 19.0   │\n",
       "└───────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m             Model Scores             \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mResult\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Accuracy                  │ 85.0   │\n",
       "│ Sensitivity               │ 81.0   │\n",
       "│ Specificity               │ 89.0   │\n",
       "│ Positive Predictive Value │ 87.0   │\n",
       "│ Negative Predictive Value │ 85.0   │\n",
       "│ False Positive Rate       │ 11.0   │\n",
       "│ False Negative Rate       │ 19.0   │\n",
       "└───────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define function to list features and their coeffecients\n",
    "def list_coef(model):\n",
    "    n_feats = model.n_features_in_\n",
    "    split_feats = np.split(model.feature_names_in_, n_feats)\n",
    "    split_coef = np.split(model.coef_[0], n_feats)\n",
    "    coef_dict = {}\n",
    "    for k, v in zip(split_feats, split_coef):\n",
    "        coef_dict[k[0]] = round(v[0], 2)\n",
    "    return coef_dict\n",
    "\n",
    "\n",
    "# Define function to run and score model\n",
    "def perform_logistic_regression(model, X, y, train= True):\n",
    "    \n",
    "    # Fit model for training, otherwise run as is with test data\n",
    "    if train == True:\n",
    "        model.fit(X, y)\n",
    "    \n",
    "    # Assign metrics\n",
    "    intercept = round(np.exp(model.intercept_)[0],4)\n",
    "    coef = list_coef(model)\n",
    "    predictions = model.predict(X)\n",
    "    accuracy = round(accuracy_score(y, predictions)*100,0)\n",
    "    \n",
    "    true_positive = sum((y == 1) & (predictions == 1))\n",
    "    true_negative = sum((y == 0) & (predictions == 0))\n",
    "    false_positive = sum((y == 0) & (predictions == 1))\n",
    "    false_negative = sum((y == 1) & (predictions == 0))\n",
    "    \n",
    "    sensitivity = round(true_positive / (true_positive + false_negative)*100, 0)\n",
    "    specificity = round(true_negative / (true_negative + false_positive)*100, 0)\n",
    "    positive_predictive_value = round(true_positive / (true_positive + false_positive)*100, 0)\n",
    "    negative_predictive_value = round(true_negative / (true_negative + false_negative)*100, 0)\n",
    "    false_positive_rate = round(false_positive / (false_positive + true_negative)*100, 0)\n",
    "    false_negative_rate = round(false_negative / (false_negative + true_positive)*100, 0)\n",
    "    \n",
    "    print(f\"{y.shape[0]} records analyzed\")\n",
    "    \n",
    "    if train == True:\n",
    "        # Prints only on 'train' since the coefficients do not change when performing 'test'\n",
    "        results_table = Table(title=\"Model Results\")\n",
    "        results_table.add_column(\"Metric\", justify=\"left\", no_wrap=True)\n",
    "        results_table.add_column(\"Result\")\n",
    "        results_table.add_row(\"Intercept\", f\"{intercept}\")\n",
    "        for co in coef:\n",
    "            results_table.add_row(f\"Coef: {co}\", f\"{coef[co]}\")\n",
    "        console.print(results_table)\n",
    "    \n",
    "    # Organized printing for metrics that matter in 'train' and 'test'\n",
    "    scores_table = Table(title=\"Model Scores\")\n",
    "\n",
    "    scores_table.add_column(\"Metric\", justify=\"left\", no_wrap=True)\n",
    "    scores_table.add_column(\"Result\")\n",
    "\n",
    "    scores_table.add_row(\"Accuracy\", f\"{accuracy}\")\n",
    "    scores_table.add_row(\"Sensitivity\", f\"{sensitivity}\")\n",
    "    scores_table.add_row(\"Specificity\", f\"{specificity}\")\n",
    "    scores_table.add_row(\"Positive Predictive Value\", f\"{positive_predictive_value}\")\n",
    "    scores_table.add_row(\"Negative Predictive Value\", f\"{negative_predictive_value}\")\n",
    "    scores_table.add_row(\"False Positive Rate\", f\"{false_positive_rate}\")\n",
    "    scores_table.add_row(\"False Negative Rate\", f\"{false_negative_rate}\")\n",
    "    \n",
    "    \n",
    "    console.print(scores_table)\n",
    "    \n",
    "    return None\n",
    "\n",
    "perform_logistic_regression(log_model, X_train, y_train, train= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the metrics above. \n",
    "\n",
    "#### Intercept\n",
    "This measures the odds of an event ocurring versus not when all coefficients are 0. \n",
    "\n",
    "Intercepts above one signify the event is more likely to happen and the vice versa. Because the intercept is just above 1, it means that the odds of heart disease being present is slightly higher than not. This doesn't mean that everyone is more likely to have heart disease, just in relation to these coefficients and this dataset. \n",
    "\n",
    "#### Coefficients and Odds Ratios\n",
    "Coef: In a logistic regression, the odds ratio represents how the outcome changes with a change in the predictor. We can isolate the coefficient (or slope) by subtracting two equations: one where $X = 0$, and one where $X = 1$, as shown below:\n",
    "\n",
    "$$log(\\frac{EY}{1-EY}) = log(O_0) = \\beta_0$$\n",
    "\n",
    "$$log(\\frac{EY}{1-EY}) = log(O_1) = \\beta_0 + \\beta_1$$\n",
    "\n",
    "where $O_0$ represents the odds when $X = 0$ and $O_1$ represents the odds when $X = 1$. Subtracting the two returns:\n",
    "\n",
    "$$log(O_1)- log(O_0) = \\beta_1$$\n",
    "$$=> log(\\frac{O_1}{O_0}) = \\beta_1$$\n",
    "\n",
    "Resulting with the coefficient being represented by the **log odds ratio**. We can then express the odds ratio in terms of a function of the slope\n",
    "\n",
    "$$(\\frac{O_1}{O_0})=e^{\\beta_1}$$\n",
    "\n",
    "If the odds ratio is greater than 1, it implies that the odds the event occurring are greater than the odds of the event not occurring. This translates into a higher coefficient. \n",
    "\n",
    "We can then calculate how these affect the The five features with the highest impact are `chest_pain_4`, `thalassemia_7.0`, `colored_arteries`, `male`, and `exercise_induced_pain`. \n",
    "\n",
    "- `chest_pain_4`: $e^{1.33} = 3.78$ >> Having asymptomatic angina multiplies odds by 3.78 (increases by 278%)\n",
    "- `thalassemia_7`: $e^{1.21} = 3.35$ >> Having \"reverseable defect\" thalassemia multiplies odds by 3.35 (increases by 235%)\n",
    "- `colored_arteries`: $e^{1.09} = 2.97$ >> One SD (Standard Deviation) increase in colored arteries multiplies odds by 2.97 (increases by 197%)\n",
    "- `male`: $e^{1.07} = 2.92$ >> Being male multiplies odds by 2.92 (increases by 192%)\n",
    "- `exercise_induced_pain`: $e^{0.76} = 2.14$ >> Having exercise-induced pain multiplies odds by 2.14 (increases by 114%)\n",
    "\n",
    "#### Accuracy\n",
    "Accuracy measures the overall correct predictions of the model. This model accurately predicted the presence of heart disease 85% of the time.\n",
    "\n",
    "#### Sensitivity and specificity\n",
    "Sensitivity and specificity measure how well the model classifies case and non-case values respectively. This model correctly identified 81% of positive cases and 89% of negative cases.\n",
    "\n",
    "#### Predictive Values\n",
    "Positive Predictive and Negative Predictive Values (PPV, NPV) measure the percentage of postive and negative predictions that were accurate. When the model predicted a postive case vs a negative case, it was right 87% of the time vs 85% of the time respectively. \n",
    "\n",
    "#### False Rates\n",
    "False Positive and False Negative Rates (FPR, FNR) measure the likelihood that a positive or negative classification is inaccurate. With a false postive Given the severity of heart disease we would rather have a higher FPR (11%) than FNR (19%), but aiming for that higher rate comes with the risk of overfitting. \n",
    "\n",
    "Overall, this model appears to be accurate and suggests that it will work well with new data instead of under or overfitting.\n",
    "\n",
    "### Testing the Model and Analyzing Results <a name= \"test\"></a>\n",
    "\n",
    "Now that we have analyzed the results and are confident with the accuracy of the model, let\"s run it on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 records analyzed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">             Model Scores             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric                    </span>┃<span style=\"font-weight: bold\"> Result </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Accuracy                  │ 89.0   │\n",
       "│ Sensitivity               │ 86.0   │\n",
       "│ Specificity               │ 93.0   │\n",
       "│ Positive Predictive Value │ 91.0   │\n",
       "│ Negative Predictive Value │ 88.0   │\n",
       "│ False Positive Rate       │ 7.0    │\n",
       "│ False Negative Rate       │ 14.0   │\n",
       "└───────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m             Model Scores             \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mResult\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Accuracy                  │ 89.0   │\n",
       "│ Sensitivity               │ 86.0   │\n",
       "│ Specificity               │ 93.0   │\n",
       "│ Positive Predictive Value │ 91.0   │\n",
       "│ Negative Predictive Value │ 88.0   │\n",
       "│ False Positive Rate       │ 7.0    │\n",
       "│ False Negative Rate       │ 14.0   │\n",
       "└───────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_logistic_regression(log_model, X_test, y_test, train= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The model performed well on the test data set, with scores increasing across the board. But can go further. With hyperparameter tuning, we can hone in to the best model settings for improved performance.\n",
    "\n",
    "## Tuning the Model\n",
    "[Back to table of contents](#toc)\n",
    "\n",
    "To find the best version of the model, we will use GridSearchCV for hyperparameter tuning. GridSearchCV tries many different combinations of a model to find the settings that return the best results so we can create the most accurate model. Lets discuss the parameters we will be tuning.\n",
    "\n",
    "- penalty: L1 (Lasso), L2 (Ridge)\n",
    "    - Lasso: \"The penalty is the sum of the absolute values of the parameters. This can lead to a sparse model where some parameter values are exactly zero, effectively removing those features from the model.\"\n",
    "    - Ridge: \"The penalty is the sum of the squares of the parameters. This evenly distributes the penalty among all parameters, shrinking them towards zero but not exactly zeroing any.\"\n",
    "- C: The inverse of regularization strength. Regularization modifies the loss function such that it encourages the model to keep coefficients small and prevent overfitting. A higher C value results in less regularization and vice versa. The optimal C value will ensure we neither underfit or overfit the data. \n",
    "- fit_intercept: Specifies if a constant (a.k.a. bias or intercept) should be added the decision function. No intercept would reduce bias in the model.\n",
    "- solver: Algorithm used in the optimization problem\n",
    "\n",
    "This [source](https://www.simplilearn.com/tutorials/machine-learning-tutorial/regularization-in-machine-learning#:~:text=The%20general%20form%20of%20the,keeping%20the%20model%20parameters%20small.) was helpful for learning more about these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': np.float64(0.1), 'fit_intercept': True, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# List parameters to try during Grid Search Cross Validation\n",
    "parameters = {\"penalty\":[\"l1\",\"l2\"], \"C\":np.logspace(-3,3,7), \"fit_intercept\":[True, False], \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\"]}\n",
    "\n",
    "# Apply to log_model and find best paramete values\n",
    "log_model_cv = GridSearchCV(log_model, param_grid=parameters, cv=10)\n",
    "log_model_cv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {log_model_cv.best_params_}\")\n",
    "print(f\"Accuracy: {round(log_model_cv.best_score_,2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tuned parameter combinations, we will create, train, and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 records analyzed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              Model Results              </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric                       </span>┃<span style=\"font-weight: bold\"> Result </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Intercept                    │ 0.2888 │\n",
       "│ Coef: age                    │ -0.07  │\n",
       "│ Coef: male                   │ 0.47   │\n",
       "│ Coef: resting_blood_pressure │ 0.2    │\n",
       "│ Coef: cholesterol            │ 0.15   │\n",
       "│ Coef: max_heart_rate         │ -0.45  │\n",
       "│ Coef: exercise_induced_pain  │ 0.46   │\n",
       "│ Coef: st_depression          │ 0.25   │\n",
       "│ Coef: colored_arteries       │ 0.69   │\n",
       "│ Coef: chest_pain_2           │ 0.01   │\n",
       "│ Coef: chest_pain_3           │ -0.42  │\n",
       "│ Coef: chest_pain_4           │ 0.65   │\n",
       "│ Coef: rest_ecg_2             │ 0.08   │\n",
       "│ Coef: st_slope_2             │ 0.33   │\n",
       "│ Coef: thalassemia_7.0        │ 0.68   │\n",
       "└──────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m              Model Results              \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mResult\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Intercept                    │ 0.2888 │\n",
       "│ Coef: age                    │ -0.07  │\n",
       "│ Coef: male                   │ 0.47   │\n",
       "│ Coef: resting_blood_pressure │ 0.2    │\n",
       "│ Coef: cholesterol            │ 0.15   │\n",
       "│ Coef: max_heart_rate         │ -0.45  │\n",
       "│ Coef: exercise_induced_pain  │ 0.46   │\n",
       "│ Coef: st_depression          │ 0.25   │\n",
       "│ Coef: colored_arteries       │ 0.69   │\n",
       "│ Coef: chest_pain_2           │ 0.01   │\n",
       "│ Coef: chest_pain_3           │ -0.42  │\n",
       "│ Coef: chest_pain_4           │ 0.65   │\n",
       "│ Coef: rest_ecg_2             │ 0.08   │\n",
       "│ Coef: st_slope_2             │ 0.33   │\n",
       "│ Coef: thalassemia_7.0        │ 0.68   │\n",
       "└──────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">             Model Scores             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric                    </span>┃<span style=\"font-weight: bold\"> Result </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Accuracy                  │ 85.0   │\n",
       "│ Sensitivity               │ 76.0   │\n",
       "│ Specificity               │ 92.0   │\n",
       "│ Positive Predictive Value │ 89.0   │\n",
       "│ Negative Predictive Value │ 82.0   │\n",
       "│ False Positive Rate       │ 8.0    │\n",
       "│ False Negative Rate       │ 24.0   │\n",
       "└───────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m             Model Scores             \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mResult\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Accuracy                  │ 85.0   │\n",
       "│ Sensitivity               │ 76.0   │\n",
       "│ Specificity               │ 92.0   │\n",
       "│ Positive Predictive Value │ 89.0   │\n",
       "│ Negative Predictive Value │ 82.0   │\n",
       "│ False Positive Rate       │ 8.0    │\n",
       "│ False Negative Rate       │ 24.0   │\n",
       "└───────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a new logistic regression model with the best parameters\n",
    "tuned_log = LogisticRegression(C= 0.1, fit_intercept= True, penalty= \"l2\", solver= \"newton-cg\", \n",
    "                              random_state=24)\n",
    "\n",
    "perform_logistic_regression(tuned_log, X_train, y_train, train= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Effects of Tuning <a name= \"tune_effects\"></a>\n",
    "\n",
    "The same five features have the highest impact as in the untuned model, but the lower C value (higher regularization) has resulted in smaller coeffeficients to avoid overfitting. Let's see how tuning the model has changed the size of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dictionaries of coefficient values into dataframes to combine\n",
    "untuned_coef= pd.DataFrame.from_dict(list_coef(log_model), orient= \"index\", columns= [\"untuned\"])\n",
    "tuned_coef= pd.DataFrame.from_dict(list_coef(tuned_log), orient= \"index\", columns= [\"tuned\"])\n",
    "\n",
    "# Create a df of coefficients to show change with tuning\n",
    "coef_df = pd.concat([untuned_coef, tuned_coef], axis= 1)\n",
    "coef_df[\"delta\"] = round((coef_df[\"tuned\"] - coef_df[\"untuned\"])/coef_df[\"untuned\"]*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matplotlib figure\n",
    "fig1, ax1 = plt.subplots(figsize= (12,6))\n",
    "\n",
    "# Plot changes to coefficients\n",
    "sns.barplot(x= coef_df[\"delta\"], y= coef_df.index, color= \"b\", ax= ax1)\n",
    "\n",
    "# Improve legibility\n",
    "ax1.set_title(\"Tuning Change to Coeffecients\", fontsize= \"x-large\")\n",
    "ax1.set_yticklabels(coef_df.index)\n",
    "ax1.set_xlabel(xlabel= \"% Change\", fontsize= \"large\")\n",
    "ax1.set_xlim(-110,35)\n",
    "ax1.tick_params(labelbottom= False)\n",
    "ax1.grid(False)\n",
    "ax1.axvline(x=0, color=\"black\", linewidth=0.8)\n",
    "for i in ax1.containers:\n",
    "    ax1.bar_label(i,rotation=15)\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure instead of displaying it inline for memory efficiency\n",
    "plt.savefig(\"images/tuning_coefficients.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Clear the plot to free memory\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://raw.githubusercontent.com/XanBauer/predicting_heart_disease/main/images/tuning_coefficients.png\" width=\"1000\" height=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above, we can see that tuning has decreased the effect sizes of all but one coefficient, meaning the model is less likely to be overfitted to the training data. \n",
    "\n",
    "`st_depression` was the only coefficient that increased, at 25%. This makes sense as we saw earlier that it can indicate potentially [life threatening disease](https://www.medicalnewstoday.com/articles/st-depression-on-ecg#:~:text=An%20ECG%20is%20a%20test,reviewed%20on%20April%2025%2C%202024).\n",
    "\n",
    "`chest_pain_2` (presence of [atypical chest pain](https://my.clevelandclinic.org/health/symptoms/24935-atypical-chest-pain)) decreased the most at -98%. This makes sense since atypical chest pain is not as likely to be associated with heart disease as the other types of chest pain. \n",
    "\n",
    "Next, let's investigate how each coefficient affects the odds of having heart disease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate log odds of coefficients into legible number\n",
    "tuned_coef[\"odds change\"] = round((e**tuned_coef[\"tuned\"] - 1) * 100,0)\n",
    "tuned_coef[\"absolute odds\"] = abs(tuned_coef[\"odds change\"])\n",
    "\n",
    "# Sort coefficients by most to least impactful \n",
    "tuned_coef = tuned_coef.sort_values(by= \"absolute odds\", axis= 0, ascending= False)\n",
    "\n",
    "# Map coefficient types for clearer visualization\n",
    "tuned_coef = tuned_coef.reset_index().rename(columns={\"index\": \"coefficient name\"})\n",
    "tuned_coef[\"coefficient type\"] = tuned_coef[\"coefficient name\"].apply(lambda x: \"Binary\" if x in cat_feats else \"Scalar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize matplotlib figure\n",
    "fig2, ax2 = plt.subplots(figsize= (12,6))\n",
    "\n",
    "# Plot effects of coefficients on presence of heart disease\n",
    "sns.barplot(data= tuned_coef, x= \"odds change\", y=\"coefficient name\", \n",
    "            hue=\"coefficient type\", dodge=False, palette=[\"#597dbf\", \"#dd8452\"], ax= ax2)\n",
    "\n",
    "# Improve legibility\n",
    "ax2.set_title(\"Coefficient Effects (% Change to Odds)\", fontsize= \"x-large\")\n",
    "ax2.set_xlabel(\"Effect Size\", fontsize= \"large\")\n",
    "ax2.set_ylabel(\"\")\n",
    "ax2.tick_params(labelbottom= False)\n",
    "ax2.axvline(x=0, color=\"black\", linewidth=0.8)\n",
    "ax2.grid(False)\n",
    "for i in ax2.containers:\n",
    "    ax2.bar_label(i)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure instead of displaying it inline for memory efficiency\n",
    "plt.savefig(\"images/coefficient_effects.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Clear the plot to free memory\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://raw.githubusercontent.com/XanBauer/predicting_heart_disease/main/images/coefficient_effects.png\" width=\"1000\" height=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plot above shows how a coefficient affects the odds of having heart disease, with the most impactful at the top and least impactful on the bottom. \n",
    "\n",
    "For scalar values (blue), the coefficient is measured in SD (standard deviation) because the data was standard scaled. For example, one SD increase in `colored_arteries` increases the odds of having heart disease by 99%. \n",
    "\n",
    "For binary values (orange), the coefficient is either present or not. For example, the presence of `chest_pain_3` decreases the odds of having heart disease by 34%. \n",
    "\n",
    "Next, let's investigate how test scores improved from the untuned vs tuned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 records analyzed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">             Model Scores             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Metric                    </span>┃<span style=\"font-weight: bold\"> Result </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Accuracy                  │ 91.0   │\n",
       "│ Sensitivity               │ 86.0   │\n",
       "│ Specificity               │ 95.0   │\n",
       "│ Positive Predictive Value │ 94.0   │\n",
       "│ Negative Predictive Value │ 89.0   │\n",
       "│ False Positive Rate       │ 5.0    │\n",
       "│ False Negative Rate       │ 14.0   │\n",
       "└───────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m             Model Scores             \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mMetric                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mResult\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│ Accuracy                  │ 91.0   │\n",
       "│ Sensitivity               │ 86.0   │\n",
       "│ Specificity               │ 95.0   │\n",
       "│ Positive Predictive Value │ 94.0   │\n",
       "│ Negative Predictive Value │ 89.0   │\n",
       "│ False Positive Rate       │ 5.0    │\n",
       "│ False Negative Rate       │ 14.0   │\n",
       "└───────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perform_logistic_regression(tuned_log, X_test, y_test, train= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df of scores to show change with tuning\n",
    "scores_df = pd.DataFrame(data= {\"untuned\": [89, 86, 93, 91, 88, 7, 14], \"tuned\": [91, 86, 95, 94, 89, 5, 14]},\n",
    "                         index=[\"Accuracy\", \"Sensitivity\", \"Specificity\", \"Positive Predictive Value\", \"Negative Predictive Value\", \"False Positive Rate\", \"False Negative Rate\"] )\n",
    "scores_df[\"delta\"] = round((scores_df[\"tuned\"] - scores_df[\"untuned\"])/scores_df[\"untuned\"]*100,2)\n",
    "\n",
    "# Initialize matplotlib figure\n",
    "fig3, ax3 = plt.subplots(figsize= (12,8))\n",
    "\n",
    "# Plot data\n",
    "sns.barplot(x= scores_df[\"delta\"], y= scores_df.index, color= \"b\", ax= ax3)\n",
    "\n",
    "# Adjust graph for legibility\n",
    "ax3.set_title(\"Tuning's Change to Scores\", fontsize= \"x-large\")\n",
    "ax3.set_yticklabels(scores_df.index, fontsize= \"large\")\n",
    "ax3.set_xlabel(xlabel= \"% Change\", fontsize= \"large\")\n",
    "ax3.set_xlim(-47,20)\n",
    "ax3.axvline(x=0, color=\"black\", linewidth=0.8)\n",
    "ax3.grid(False)\n",
    "for i in ax3.containers:\n",
    "    ax3.bar_label(i,)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure instead of displaying it inline for memory efficiency\n",
    "plt.savefig(\"images/tuning_changes.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Clear the plot to free memory\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://raw.githubusercontent.com/XanBauer/predicting_heart_disease/main/images/tuning_changes.png\" width=\"750\" height=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the model improved 5/7 of the scores. The biggest improvement was a nearly 30% decrease in the false positive rate. That means 30% less cases where someone is unnecessarily scared at the hospital when getting tested with this model. The other scores increased only slightly, but in a matter as serious as heart disease, that means more lives saved. \n",
    "\n",
    "## Conclusion <a name= \"conclusion\"></a>\n",
    "[Back to table of contents](#toc)\n",
    "\n",
    "#### Relationships\n",
    "There were some surprises in the relationship found with this data. For example, this model showed an increase in age to result in a decreased odds of having heart disease. I would expect the opposite, as medical problems tend to increase with age. Another surprise was asymptomatic angina (`chest_pain_4`) having a 92% increase in the odds of having heart disease. It's possible that asymptomatic angina does not mean the pure absence of heart pain, or maybe most people when being diagnosed with heart disease for the first time do not have chest pain. \n",
    "\n",
    "#### Scores\n",
    "The model correctly predicted 91% of all instances. While We can be confident when using this to screen patients for heart disease. \n",
    "\n",
    "The model was better at identifying non-case than case values (95% vs 86%), which could be due to the dataset being biased towards non-case values (164 non-case vs 139 case).\n",
    "\n",
    "Despite the class imbalance in the dataset, the model was more accurate when predicting case (94%) vs non-case (89%) values.\n",
    "\n",
    "Looking at the [Baseline Model Performance](https://archive.ics.uci.edu/dataset/45/heart+disease), we see that this model performs above the average baseline performance for logistic regression models (91% vs 82%). When comparing precision (positive predictive value), this model also performs above the average baseline performance for logistic regression models (94% vs 83%). \n",
    "\n",
    "Overall, this model has strong accuracy and seems acceptable for use in a clinical setting. Should there be other signs at odds with the results of this model, further tests are warranted to ensure an approrpriate diagnosis. However, this model could save lots of time, money, and lives. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
